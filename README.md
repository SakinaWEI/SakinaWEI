## ğŸ‘‹æ¬¢è¿æ¥åˆ°æˆ‘çš„GitHubä¸»é¡µï¼

æˆ‘æ˜¯Sakinaï¼Œç›®å‰åœ¨ [GLM-4.5 å›¢é˜Ÿ](https://z.ai/blog/glm-4.5)ï¼Œæ­£åœ¨å¯»æ‰¾ TOP Talents åŠ å…¥æˆ‘ä»¬ ï½
Join us! Top Talents for AGI, Global Hiringï¼ï¼

About our team:
Our world-leading AI team has developed the cutting-edge large language and multimodal models and built the high-precision billion-scale knowledge graphs, the combination of which uniquely empowers us to create a powerful data- and knowledge-driven cognitive engine towards AGI. 

- Email: shuangshuang.wei@zhipuai.cn
- Wechat: SakinaWEI

**LLM Research Scientist/Engineer**

**Responsibilities:**

- Design and deploy state-of-the-art NLP/Multimodal LLM

- Research areas include, but are not limited to, efficient large language model architecture, multimodal learning, self-supervised representation learning, unified cross-task learning, dataset construction, RLHF, etc.

**Qualifications:**

- PhD/Master in Computer Science, Artificial Intelligence, or a related field.

- Solid research accumulation in natural language understanding, machine learning, deep learning, and multimodal domains.

- Excellent large model research capabilities, with a preference for those who have published high-quality papers in top conferences such as NeurIPS, ICLR, ICML, ACL, EMNLP, CVPR, JMLR, etc.

- Outstanding collaborative abilities, able to coordinate with platform, data, and other teams to complete systematic work, excellent direction planning and implementation capabilities.

**ML System Research Scientist/Engineer**

**Responsibilities:**

- Lead the creation of next-generation, high-capacity LLM platforms.

- Collaborate with software engineers to build platforms with cutting-edge models.

**Qualifications:**

- PhD/Master in Computer Science, Artificial Intelligence, or a related field.

- Have prior experience working with training and inference of large language models.

- Have experience in High performance, large-scale ML systems, GPUs, Kubernetes, Pytorch, or OS internals

- Proficiency in programming languages such as Python or C++ and a track record of working with deep learning frameworks (e.g., pytorch, deepspeed, etc.).

- Strong understanding of distributed computing framework & performance tuning and verification for training/finetuning/inference. 

- Being familiar with PEFT or MoE is a plus.

å¦‚æœæ‚¨å¯¹æˆ‘çš„é¡¹ç›®æˆ–å·¥ä½œæœ‰ä»»ä½•ç–‘é—®æˆ–å»ºè®®ï¼Œè¯·éšæ—¶ä¸æˆ‘è”ç³»ğŸ˜Š~
